{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiJIyOTVvXP/OsnrJ4yEWZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prakhar021-hub/Natural-Language-Processing/blob/main/Text_Preprocessing_in_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxDh-0P9BRtZ",
        "outputId": "62f46956-c27e-4642-e8a9-95df3997f6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvKTcYImBbfk",
        "outputId": "ed46043d-1f40-481e-b7a3-2131c4feb359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample\n",
        "\n",
        "text_word = \"You are studying NLP article\"\n",
        "\n",
        "sentence_word = \"learning pyhton is very easy. I love python programming \""
      ],
      "metadata": {
        "id": "e-3zw7P9Bbh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_word = word_tokenize(text_word)\n",
        "print(token_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvG7UhpxBbkM",
        "outputId": "fcef0a8e-c763-4f12-daa8-13b1d0cbf96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You', 'are', 'studying', 'NLP', 'article']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_sent = sent_tokenize(sentence_word)\n",
        "print(token_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMO60tBDBbmE",
        "outputId": "019cbea5-3b6c-4d57-97f8-b91f6a61931b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learning pyhton is very easy.', 'I love python programming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## we can tokenize the words of each sentence to reduce computation for larger sentences\n",
        "\n",
        "for i in token_sent:\n",
        "  x = word_tokenize(i)\n",
        "  print(x)"
      ],
      "metadata": {
        "id": "SCRxtJoPBbpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e8e237-ceae-4384-d2c7-1104b4eb2efc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learning', 'pyhton', 'is', 'very', 'easy', '.']\n",
            "['I', 'love', 'python', 'programming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stopword removal on the tokenize sentence\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHz37QTehfJ8",
        "outputId": "e849021a-cf2e-4cd7-9708-0e21e738705d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word = set(stopwords.words(\"english\"))\n",
        "print(stop_word)\n",
        "print(len(stop_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND_JIbA7iSKf",
        "outputId": "8d3ea0fc-431c-4fe6-c964-a9ba74d04926"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'at', 'has', \"he'd\", 'nor', 'who', 'now', 'so', 'couldn', \"wouldn't\", \"wasn't\", 'some', \"it's\", 'haven', 'other', 'they', 'she', \"they're\", 'below', \"hasn't\", \"couldn't\", \"i've\", 'hasn', 'ours', 'of', 'while', 'himself', 'be', 'out', \"it'd\", 'needn', \"you'd\", 'can', 'ma', 'than', \"shouldn't\", 'hadn', 'each', 'his', 'very', 'mustn', 'why', 'will', 'yourself', 'where', 'do', 'from', 'in', 're', 'didn', 'mightn', 'after', 'is', 'up', 'before', 'am', 'herself', 'an', 'both', 'not', 'shouldn', 'll', \"needn't\", 'our', 'during', 'between', 'doesn', 'themselves', \"doesn't\", \"isn't\", \"they'd\", 'then', 'yourselves', 'myself', 'd', 'off', 'have', 'weren', 'few', \"you'll\", \"i'll\", 'don', 'and', \"he's\", 'm', \"weren't\", 'him', 'above', 'its', 'again', 'down', 'a', \"it'll\", 'aren', 'more', 'against', 'been', 'to', \"we'll\", 's', 'on', \"they'll\", 'most', 'only', 'ain', \"she'll\", \"you've\", 'until', 'when', 'because', 'was', 'having', \"didn't\", 'her', 'wouldn', 'further', 'over', 'wasn', 'those', 'into', \"that'll\", \"she'd\", 'about', 'itself', \"haven't\", \"aren't\", \"i'm\", 'as', 'there', 'here', 'you', 'your', 'same', \"don't\", \"she's\", 'under', 'any', 'their', 'my', 'once', 'too', \"should've\", \"we're\", 'all', \"we'd\", 'had', 'me', 'were', \"we've\", \"won't\", 'y', \"you're\", 'yours', \"hadn't\", 'hers', 'what', 've', 't', 'we', 'by', \"shan't\", \"mightn't\", 'shan', 'being', 'doing', 'i', 'such', \"they've\", 'did', 'ourselves', 'won', 'he', 'just', \"he'll\", 'o', 'own', 'the', 'isn', 'theirs', 'through', 'which', 'for', \"i'd\", 'this', 'whom', 'but', 'these', 'are', 'does', 'them', 'or', 'how', 'if', \"mustn't\", 'with', 'that', 'it', 'no', 'should'}\n",
            "198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### removing the stopwords\n",
        "\n",
        "text = \"so this is some Random text generated ,just for the practice purpose only. \"\n",
        "tokens = word_tokenize(text.lower())\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scCoEdCTiXKE",
        "outputId": "7de62db0-be8a-41da-d580-b61f9d012cfc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['so', 'this', 'is', 'some', 'random', 'text', 'generated', ',', 'just', 'for', 'the', 'practice', 'purpose', 'only', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_text = [i for i in tokens if i not in stop_word]\n",
        "print(updated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL6pjvLrlzEU",
        "outputId": "4ce7cf15-c8dc-4c7b-dff9-54b851a1711c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['random', 'text', 'generated', ',', 'practice', 'purpose', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## lower casing  : you can just use python built in function lower() to lower case all the words of the sentence\n",
        "\n",
        "x = \"hello , This is Python  TUTORIAL\"\n",
        "x.lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y25FzlIlmCOW",
        "outputId": "48f5cede-4b99-405e-99eb-4875fe5d9454"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello , this is python  tutorial'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## removing the punctuations\n",
        "\n",
        "import string\n",
        "string.punctuation\n",
        "\n",
        "sentence = \"hello!! everyone, I am Prakhar, just Prakhar, hope you all are doing well. I am sharing my experience in learning programmin.\"\n"
      ],
      "metadata": {
        "id": "z1NdM2WCmvUx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation  = str.maketrans(\"\",\"\", string.punctuation )"
      ],
      "metadata": {
        "id": "I6UQxf8qocsy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_sentence = sentence.translate(translation)\n",
        "print(processed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJSzpjS3ohWo",
        "outputId": "73c70a5a-5b2e-451c-a608-0f2e70755b44"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello everyone I am Prakhar just Prakhar hope you all are doing well I am sharing my experience in learning programmin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "41Yih-xWo6Wb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = [\"running\", \"swimming\", \"burnt\", \"rationality\", \"happily\", \"lovey\", \"eating\", \"slept\"]\n",
        "stemmer = PorterStemmer()\n",
        "updated_sample_words =  [stemmer.stem(i) for i in sample_words]\n",
        "print(updated_sample_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtp5JaSssjKo",
        "outputId": "031d2ebd-e4dd-4555-8778-719f794b4c4d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'swim', 'burnt', 'ration', 'happili', 'lovey', 'eat', 'slept']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## POS tagging\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13C0YYxstQ2K",
        "outputId": "f4584c73-5df2-4058-87e2-5d83f6177d19"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = \"the quick brown fox jumps over the lazy dog\"\n",
        "tkns = word_tokenize(some_text)\n",
        "print(tkns)\n",
        "\n",
        "tags = pos_tag(tkns)\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H0ATeNOukQX",
        "outputId": "e2cd18d1-f858-495d-97d8-54a59e46a0e9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lemmatization using spacy and nltk\n",
        "\n",
        "!pip install spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om8KYkVExaR-",
        "outputId": "73886c0a-0ec7-4ca2-b2e4-cff63f8e2d12"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using spacy\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "words = [\"running\", \"swimming\", \"flying\", \"sleeping\", \"slapped\", \"burnt\"]\n",
        "for word in words:\n",
        "    doc = nlp(word)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} -> {token.lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0g63Dfqvf0T",
        "outputId": "8e1ea18f-51a6-4243-80c4-2f2e4d31979d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "swimming -> swim\n",
            "flying -> fly\n",
            "sleeping -> sleep\n",
            "slapped -> slap\n",
            "burnt -> burn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## using nltk  (my personal opiniun spacy is better )\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"swimming\", \"flying\", \"sleeping\", \"slapped\", \"burnt\"]\n",
        "\n",
        "# POS tagging\n",
        "from nltk import pos_tag\n",
        "tags = pos_tag(words)\n",
        "\n",
        "# Lemmatize with correct POS\n",
        "for word, pos in tags:\n",
        "    wn_pos = get_wordnet_pos(pos)\n",
        "    lemma = lemmatizer.lemmatize(word, wn_pos)\n",
        "    print(f\"{word} -> {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcZWGN2HxlL7",
        "outputId": "3f92d679-a996-419e-ac1f-ca41f452e355"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "swimming -> swim\n",
            "flying -> fly\n",
            "sleeping -> sleep\n",
            "slapped -> slap\n",
            "burnt -> burnt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGjUBSd-yOe_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}